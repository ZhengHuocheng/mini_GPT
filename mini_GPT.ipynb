{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "Cw1lGOJHLB8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKlzB-PYYzX",
        "outputId": "e74e961e-3433-442e-d71d-14b46ebf36f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-31 05:07:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-31 05:07:03 (18.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 下载tiny karpathy dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 基于sub_word(there is per char) 构建简单的字符集tokrnizer映射\n",
        "with open('input.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "bK38GUoTY32A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = sorted(list(set(text)))\n",
        "vocab_size = len(char_set)"
      ],
      "metadata": {
        "id": "Wdufws78ZfwY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzh-OQBGZ5oN",
        "outputId": "cdca011f-059f-4844-8eef-95d55164b1a6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(char_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cvSHzUKucUA4",
        "outputId": "c36139ae-ceb8-47b8-9777-9985feff2f24"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size #可以设置corpus的键为65个"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGAD3tVpBa6y",
        "outputId": "f75fae1d-8e96-4ab0-b98f-892ec98849fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\n",
        "simple strategy for tokenize the input\n",
        "\"\"\"\n",
        "str2num = {ch:i for i,ch in enumerate(char_set)}\n",
        "num2str = {i:ch for i,ch in enumerate(char_set)}\n",
        "\n",
        "encode = lambda s: [str2num[ch] for ch in s] #encoder:translate a string to a list of int\n",
        "decode = lambda l: ''.join([num2str[i] for i in l]) #docoder:translate a list of int to output:string"
      ],
      "metadata": {
        "id": "V4okBC6xBmkz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"nihao i love u\"))\n",
        "print('\\n')\n",
        "print(decode(encode(\"nihao i love u\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsn-UgD-DBUI",
        "outputId": "e5fd9ca9-a5a4-4c2e-8225-ee50f97fd615"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[52, 47, 46, 39, 53, 1, 47, 1, 50, 53, 60, 43, 1, 59]\n",
            "\n",
            "\n",
            "nihao i love u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将整个dataset进行tokenize\n",
        "import torch\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data.shape,data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5EPA9EJDUTS",
        "outputId": "5d14f8a2-b35b-4dd6-9489-0dd3e79c73b7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 划分90%训练集和10%验证集\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data),len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr7FH2XcMqPy",
        "outputId": "0e0c60ea-0075-459c-f0b9-3fbf37d91bd7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "随机采样一定的数据来输入Transformer中，避免一次性输入"
      ],
      "metadata": {
        "id": "AY-jVxQRTO-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "# wandb.init(project=\"Mini_GPT\")\n",
        "# config = wandb.config\n",
        "max_size_per_token_input = 8\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "lr=1e-3\n",
        "dropout = 0.2\n",
        "n_embd = 32\n",
        "n_layer = 4\n",
        "n_head = 4"
      ],
      "metadata": {
        "id": "anlpRIffLO05"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_size_per_token_input = 8\n",
        "# 进行CBOW，预测末位token\n",
        "x = train_data[:max_size_per_token_input]\n",
        "y = train_data[1:max_size_per_token_input+1]\n",
        "for _ in range(max_size_per_token_input):\n",
        "  context = x[:_+1]\n",
        "  target = y[_]\n",
        "  print(f\"input context {context},target {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqSC1m82NwlN",
        "outputId": "ff89c3d3-de80-411c-f4e5-947ef49d5926"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input context tensor([18]),target 47\n",
            "input context tensor([18, 47]),target 56\n",
            "input context tensor([18, 47, 56]),target 57\n",
            "input context tensor([18, 47, 56, 57]),target 58\n",
            "input context tensor([18, 47, 56, 57, 58]),target 1\n",
            "input context tensor([18, 47, 56, 57, 58,  1]),target 15\n",
            "input context tensor([18, 47, 56, 57, 58,  1, 15]),target 47\n",
            "input context tensor([18, 47, 56, 57, 58,  1, 15, 47]),target 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)  #设置随机种子，方便复现\n",
        "batch_size = 4  #并行输入的独立序列，batch容量\n",
        "block_size = 8  #context最大token\n",
        "\n",
        "def get_batch(dstype):\n",
        "  data = train_data if dstype == 'train' else val_data\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,)) #size_template 1dim (batch_size=4,)\n",
        "  # print(ix)\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y #每个batch中sequence对应的context和target\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pueCuwtUUZjS",
        "outputId": "fc136d29-495a-4608-fe13-26a510b9dd3d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(batch_size):\n",
        "  for i in range(block_size):\n",
        "    context = xb[_,:i+1]\n",
        "    target = yb[_,i]\n",
        "    print(f\"sequence{_}：input context {context},target {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmzjwuCOZfWS",
        "outputId": "89159892-0c12-4c8f-a987-753c06f33ea0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence0：input context tensor([24]),target 43\n",
            "sequence0：input context tensor([24, 43]),target 58\n",
            "sequence0：input context tensor([24, 43, 58]),target 5\n",
            "sequence0：input context tensor([24, 43, 58,  5]),target 57\n",
            "sequence0：input context tensor([24, 43, 58,  5, 57]),target 1\n",
            "sequence0：input context tensor([24, 43, 58,  5, 57,  1]),target 46\n",
            "sequence0：input context tensor([24, 43, 58,  5, 57,  1, 46]),target 43\n",
            "sequence0：input context tensor([24, 43, 58,  5, 57,  1, 46, 43]),target 39\n",
            "sequence1：input context tensor([44]),target 53\n",
            "sequence1：input context tensor([44, 53]),target 56\n",
            "sequence1：input context tensor([44, 53, 56]),target 1\n",
            "sequence1：input context tensor([44, 53, 56,  1]),target 58\n",
            "sequence1：input context tensor([44, 53, 56,  1, 58]),target 46\n",
            "sequence1：input context tensor([44, 53, 56,  1, 58, 46]),target 39\n",
            "sequence1：input context tensor([44, 53, 56,  1, 58, 46, 39]),target 58\n",
            "sequence1：input context tensor([44, 53, 56,  1, 58, 46, 39, 58]),target 1\n",
            "sequence2：input context tensor([52]),target 58\n",
            "sequence2：input context tensor([52, 58]),target 1\n",
            "sequence2：input context tensor([52, 58,  1]),target 58\n",
            "sequence2：input context tensor([52, 58,  1, 58]),target 46\n",
            "sequence2：input context tensor([52, 58,  1, 58, 46]),target 39\n",
            "sequence2：input context tensor([52, 58,  1, 58, 46, 39]),target 58\n",
            "sequence2：input context tensor([52, 58,  1, 58, 46, 39, 58]),target 1\n",
            "sequence2：input context tensor([52, 58,  1, 58, 46, 39, 58,  1]),target 46\n",
            "sequence3：input context tensor([25]),target 17\n",
            "sequence3：input context tensor([25, 17]),target 27\n",
            "sequence3：input context tensor([25, 17, 27]),target 10\n",
            "sequence3：input context tensor([25, 17, 27, 10]),target 0\n",
            "sequence3：input context tensor([25, 17, 27, 10,  0]),target 21\n",
            "sequence3：input context tensor([25, 17, 27, 10,  0, 21]),target 1\n",
            "sequence3：input context tensor([25, 17, 27, 10,  0, 21,  1]),target 54\n",
            "sequence3：input context tensor([25, 17, 27, 10,  0, 21,  1, 54]),target 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bi_gram Language Model  (Boliean Embedding)\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BiGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    logits = self.token_embedding_table(idx) #batch_size*block_size*vocab_size\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    \"\"\"\n",
        "    idx:(batch_size,block_size)\n",
        "    max_new_tokens:接下来需要生成的最多token数\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx) #得到预测结果\n",
        "      logits = logits[:,-1,:]  #需要得到最后的time step，shape(B,C)\n",
        "      probs = F.softmax(logits,dim=-1)  #将分布转换为概率\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #采样处最大概率的索引，(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx #shape(B,T+1)"
      ],
      "metadata": {
        "id": "ah8lI1mFa3oj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiGramLM(vocab_size)\n",
        "logits, loss = model(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bv-jlvSdOaQ",
        "outputId": "8949c587-7950-4fa6-9a23-58a576125056"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(model.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=250)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "S55KTASjfIG9",
        "outputId": "f8bc054d-7e88-436e-fb04-abc97dc0e84f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3!dcbf?pGXepydZJSrF$Jrqt!:wwWSzPNxbjPiD&Q!a;yNt$Kr$o-gC$WSjJqfBKBySKtSKpwNNfyl&w:q-jluBatD$Lj;?yzyUca!UQ!vrpxZQgC-hlkq,ptKqHoiX-jjeLJ &slERj KUsBOL!mpJ\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建优化器\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) # 自注意力不适用高的学习率，而学习率降低，则需要提高训练迭代次数，以保持性能。并且模型增大时，也需要降低学习率，来防止局部最优"
      ],
      "metadata": {
        "id": "jBPeRZZo2dro"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# 验证集，用于在训练时观察验证情况，train_loss > val_loss 说明训练正常,小于时则说明有过拟合的风险，此时观察epoch进行选择\n",
        "for epoch in range(10000+1):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 1000 == 0:\n",
        "    print(f\"epoch {epoch},loss {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMb0Qerb4DhE",
        "outputId": "8ffc18e0-b6c3-4cb4-be45-176b985977ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0,loss 4.711565017700195\n",
            "epoch 1000,loss 3.7984933853149414\n",
            "epoch 2000,loss 3.0819954872131348\n",
            "epoch 3000,loss 2.765151023864746\n",
            "epoch 4000,loss 2.633697748184204\n",
            "epoch 5000,loss 2.5281035900115967\n",
            "epoch 6000,loss 2.6245577335357666\n",
            "epoch 7000,loss 2.476557493209839\n",
            "epoch 8000,loss 2.564079999923706\n",
            "epoch 9000,loss 2.4546542167663574\n",
            "epoch 10000,loss 2.4646947383880615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = decode(model.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=1000)[0].tolist())"
      ],
      "metadata": {
        "id": "QnLpmAbb4bxt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "5He2Hq2b7LeJ",
        "outputId": "ea4e7b57-333a-46fb-9c4e-4893268592b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThixqu tes; st yo hind wotte grotonear 'so it t jod weancotha:\\nh hay.JUCle n prids, r loncave w hollular s O:\\nHIs; ht anjx?\\n\\nDUThineent.\\n\\nLavinde.\\nathave l.\\nKEONGBUCHandspo be y,-hedarwnoddy scace, tridesar, wne'shenous s ls, theresseys\\nPlorseelapinghiybHen yof GLUCEN t l-t E:\\nI hisgothers w dere! ABer wotouciullle's fldrwertho s?\\nNDan'spererds cist ripl chys er orlese;\\nYo jehof h hecere ekn wferommot mowo soaf yoit, ince his, t, f at. fal whetrimy bupof tor atha Bu!\\nJOutho f cimimave.\\nNEDUSt cir selle p wie wede\\nRo n apenor f'Y tover witys an sh d w t e w!\\nCEOntiretoaveE IINpe, theck. cung.\\nORIsthies hacin benqurd bll, d a r w wistatsowor ath\\nFivet bloll ang a-I theeancusemee tsce larry t I lag sze t\\nLCKI thit,\\nn.\\nFaure ds ppplirn!\\nmeftou ow pring, avewist th;\\nTENTEMETCI gienco, An he waro whiougou he s imaror?\\nBu ne-ingof acat nd l,\\nFothind at wrt:\\nHMovint gros!\\nAMQUThes med thestw cos wand herf s hafold mZWirus je ney biPos t ngabsestouMOLAUCES: ONDohery ththe tonty th, fourf thatys\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# self-attention module"
      ],
      "metadata": {
        "id": "bTbhRKCWSaXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建单头自注意力\n",
        "# decoder结构\n",
        "class OneHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embd,head_size,bias=False)\n",
        "    self.key = nn.Linear(n_embd,head_size,bias=False)\n",
        "    self.value = nn.Linear(n_embd,head_size,bias=False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) #利用buffer创建下三角矩阵\n",
        "    self.dropout = nn.Dropout(dropout) #全局变量中定义\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    q = self.query(x) #(B,T,head_size)\n",
        "    k = self.key(x) #(B,T,head_size)\n",
        "    v = self.value(x) #(B,T,head_size)\n",
        "    # 计算attention score\n",
        "    weight = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
        "    # 除以的根C，故也成缩放点积注意力\n",
        "    # 这里除以的根号C，是为了将分布归一化，使得输出稳定，而不会出现错峰的峰值\n",
        "    weight = weight.masked_fill(self.tril[:T,:T] == 0, float('-inf')) #添加掩码，防止未来信息泄露\n",
        "    # 对于一些不需要隐藏未来信息的任务，可以不用掩码，如情感分析\n",
        "    weight = F.softmax(weight,dim=-1)\n",
        "    weight = self.dropout(weight) # 在一定程度上妨碍计算间的通信，dropout来防止过拟合\n",
        "    out = weight @ v # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0MWxqQdM3pKW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "class BiGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    # 将cpopus进行嵌入，是对自身信息的处理\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.ln_head = nn.Linear(n_embd,vocab_size)\n",
        "    # 对位置关系进行处理\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) #batch_size*block_size*n_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #block_size*n_embd\n",
        "    x = tok_emb + pos_emb #batch_size*block_size*n_embd的2倍\n",
        "    # x此时包含了自身token信息和位置信息\n",
        "    logits = self.ln_head(x) #batch_size*block_size*vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    \"\"\"\n",
        "    idx:(batch_size,block_size)\n",
        "    max_new_tokens:接下来需要生成的最多token数\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:] #限制idx大小，防止位置嵌入表溢出\n",
        "      logits, loss = self(idx_cond) #得到预测结果\n",
        "      logits = logits[:,-1,:]  #需要得到最后的time step，shape(B,C)，即序列中的最后一个字词的得分分布\n",
        "      probs = F.softmax(logits,dim=-1)  #将分布转换为概率\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #采样处最大概率的索引，(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx #shape(B,T+1)"
      ],
      "metadata": {
        "id": "UksN7BxESA4R"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多头注意力--并行多个self-attention"
      ],
      "metadata": {
        "id": "VgU9pbn8_9wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([OneHead(head_size) for _ in range(num_heads)])  #实例化所包含的多个自注意力头\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout) #全局变量中定义\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads],dim=-1)\n",
        "    out = self.proj(out) #残差连接的中间过程，将结果投影到剩余层(路径)\n",
        "    return out"
      ],
      "metadata": {
        "id": "tEgR8SfX_9My"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "class BiGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    # 将cpopus进行嵌入，是对自身信息的处理\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.ln_head = nn.Linear(n_embd,vocab_size)\n",
        "    # 对位置关系进行处理\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    self.multihead = MultiHeadAttention(4,n_embd//4)  # 由于n_embd为32，设置的是4个注意力头，给个注意力头的输入为8-dim\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) #batch_size*block_size*n_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #block_size*n_embd\n",
        "    x = tok_emb + pos_emb #batch_size*block_size*n_embd\n",
        "    # x此时包含了自身token信息和位置信息\n",
        "    x = self.multihead(x)\n",
        "    logits = self.ln_head(x) #batch_size*block_size*vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    \"\"\"\n",
        "    idx:(batch_size,block_size)\n",
        "    max_new_tokens:接下来需要生成的最多token数\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:] #限制idx大小，防止位置嵌入表溢出\n",
        "      logits, loss = self(idx_cond) #得到预测结果\n",
        "      logits = logits[:,-1,:]  #需要得到最后的time step，shape(B,C)，即序列中的最后一个字词的得分分布\n",
        "      probs = F.softmax(logits,dim=-1)  #将分布转换为概率\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #采样处最大概率的索引，(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx #shape(B,T+1)"
      ],
      "metadata": {
        "id": "Yem5HowRCMIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前馈层--以单层MLP尝试"
      ],
      "metadata": {
        "id": "6-e-dDHIEWnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd,4*n_embd),\n",
        "        nn.ReLU(), #ReLU是通用的激活函数，已可以换位LakeReLU或tanh\n",
        "        nn.Linear(4*n_embd,n_embd), #映射到原来的残差路径\n",
        "        nn.Dropout(dropout), # 全局变量指定，一般在投影回残差连接路径前进行dropout，来将其作为最后一层\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "x_2hO22sEfeK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "class BiGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    # 将cpopus进行嵌入，是对自身信息的处理\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.ln_head = nn.Linear(n_embd,vocab_size)\n",
        "    # 对位置关系进行处理\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    self.multihead = MultiHeadAttention(4,n_embd//4)  # 由于n_embd为32，设置的是4个注意力头，给个注意力头的输入为8-dim\n",
        "    self.feed_forward = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) #batch_size*block_size*n_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #block_size*n_embd\n",
        "    x = tok_emb + pos_emb #batch_size*block_size*n_embd\n",
        "    # x此时包含了自身token信息和位置信息\n",
        "    x = self.multihead(x)\n",
        "    x = self.feed_forward(x)\n",
        "    logits = self.ln_head(x) #batch_size*block_size*vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    \"\"\"\n",
        "    idx:(batch_size,block_size)\n",
        "    max_new_tokens:接下来需要生成的最多token数\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:] #限制idx大小，防止位置嵌入表溢出\n",
        "      logits, loss = self(idx_cond) #得到预测结果\n",
        "      logits = logits[:,-1,:]  #需要得到最后的time step，shape(B,C)，即序列中的最后一个字词的得分分布\n",
        "      probs = F.softmax(logits,dim=-1)  #将分布转换为概率\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #采样出最后一个sequence中每个token的最大概率的索引，(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx #shape(B,T+1)"
      ],
      "metadata": {
        "id": "J9WGiE-RFx1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 创建Transformer的block简单结构，主要重复多头注意力"
      ],
      "metadata": {
        "id": "GPoW0dPqHeqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class block(nn.Module):\n",
        "  \"\"\"\n",
        "  这个块结构主要以进行计算间通信的多头注意力构成\n",
        "  也可以加入加MLP尝试\n",
        "  \"\"\"\n",
        "  def __init__(self,n_embd,n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.mltHead = MultiHeadAttention(n_head,head_size)\n",
        "    self.feed_forward = FeedForward(n_embd)\n",
        "    self.lnm1 = nn.LayerNorm(n_embd)\n",
        "    self.lnm2 = nn.LayerNorm(n_embd)\n",
        "  def forward(self,x):\n",
        "    # x = self.mltHead(x)\n",
        "    # x = self.feed_forward(x)\n",
        "    x = x + self.mltHead(self.lnm1(x))\n",
        "    x = x + self.feed_forward(self.lnm2(x)) #残差连接，防止梯度消失，加快收敛\n",
        "    return x"
      ],
      "metadata": {
        "id": "YPDgl9bFHeCn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "class BiGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    # 将cpopus进行嵌入，是对自身信息的处理\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.ln_head = nn.Linear(n_embd,vocab_size)\n",
        "    # 对位置关系进行处理\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    # self.multihead = MultiHeadAttention(4,n_embd//4)  # 由于n_embd为32，设置的是4个注意力头，给个注意力头的输入为8-dim\n",
        "    # 已经包含在sequential中了\n",
        "    self.blocks = nn.Sequential(\n",
        "        block(n_embd,4),\n",
        "        block(n_embd,4),\n",
        "        block(n_embd,4),\n",
        "        block(n_embd,4),\n",
        "        nn.LayerNorm(n_embd), #到解码的最后一个线性层前再进行一次\n",
        "    )\n",
        "    # self.feed_forward = FeedForward(n_embd) #已经包含在sequential中了\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) #batch_size*block_size*n_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #block_size*n_embd\n",
        "    x = tok_emb + pos_emb #batch_size*block_size*n_embd\n",
        "    # x此时包含了自身token信息和位置信息\n",
        "    # x = self.multihead(x)\n",
        "    # x = self.feed_forward(x)\n",
        "    x = self.blocks(x)\n",
        "    logits = self.ln_head(x) #batch_size*block_size*vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    \"\"\"\n",
        "    idx:(batch_size,block_size)\n",
        "    max_new_tokens:接下来需要生成的最多token数\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:] #限制idx大小，防止位置嵌入表溢出\n",
        "      logits, loss = self(idx_cond) #得到预测结果\n",
        "      logits = logits[:,-1,:]  #需要得到最后的time step，shape(B,C)，即序列中的最后一个字词的得分分布\n",
        "      probs = F.softmax(logits,dim=-1)  #将分布转换为概率\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #采样出最后一个sequence中每个token的最大概率的索引，(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx #shape(B,T+1)"
      ],
      "metadata": {
        "id": "aaC4DgOmIgj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer中存在的两个优化措施：1、残差连接 2、层间Norm Layer，现在大多是在转化前运用BatchNorm,这与论文不同"
      ],
      "metadata": {
        "id": "6LZUB7gJM3uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# 完善，模型可扩展性，使其结构能重复n次\n",
        "class BiGramLM(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    # 将cpopus进行嵌入，是对自身信息的处理\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.ln_head = nn.Linear(n_embd,vocab_size)\n",
        "    # 对位置关系进行处理\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    # self.multihead = MultiHeadAttention(4,n_embd//4)  # 由于n_embd为32，设置的是4个注意力头，给个注意力头的输入为8-dim\n",
        "    # 已经包含在sequential中了\n",
        "    self.blocks = nn.Sequential(*[block(n_embd, n_head=n_head) for _ in range(n_layer)]) #全局变量中指定\n",
        "    self.f_lnm = nn.LayerNorm(n_embd) #最后输出线性层前的最后一次layernorm\n",
        "    # self.feed_forward = FeedForward(n_embd) #已经包含在sequential中了\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) #batch_size*block_size*n_embd\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #block_size*n_embd\n",
        "    x = tok_emb + pos_emb #batch_size*block_size*n_embd\n",
        "    # x此时包含了自身token信息和位置信息\n",
        "    # x = self.multihead(x)\n",
        "    # x = self.feed_forward(x)\n",
        "    x = self.blocks(x)\n",
        "    x = self.f_lnm(x)\n",
        "    logits = self.ln_head(x) #batch_size*block_size*vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    \"\"\"\n",
        "    idx:(batch_size,block_size)\n",
        "    max_new_tokens:接下来需要生成的最多token数\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:] #限制idx大小，防止位置嵌入表溢出\n",
        "      logits, loss = self(idx_cond) #得到预测结果\n",
        "      logits = logits[:,-1,:]  #需要得到最后的time step，shape(B,C)，即序列中的最后一个字词的得分分布\n",
        "      probs = F.softmax(logits,dim=-1)  #将分布转换为概率\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #采样出最后一个sequence中每个token的最大概率的索引，(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx #shape(B,T+1)"
      ],
      "metadata": {
        "id": "YDPSBYVlM2yN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BiGramLM(vocab_size)\n",
        "# wandb.watch(model,log=\"all\")\n",
        "for epoch in range(10000+1):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 1000 == 0:\n",
        "    print(f\"epoch {epoch},loss {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aya-nFuGsNK",
        "outputId": "b1036d71-08e3-41d8-eb79-fd2489a81fb8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0,loss 4.380142688751221\n",
            "epoch 1000,loss 2.7046000957489014\n",
            "epoch 2000,loss 2.893082857131958\n",
            "epoch 3000,loss 2.5212631225585938\n",
            "epoch 4000,loss 2.1198503971099854\n",
            "epoch 5000,loss 2.2809371948242188\n",
            "epoch 6000,loss 2.1075241565704346\n",
            "epoch 7000,loss 2.289177417755127\n",
            "epoch 8000,loss 1.940026044845581\n",
            "epoch 9000,loss 2.0970635414123535\n",
            "epoch 10000,loss 2.111042022705078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPqLozqkHpip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}